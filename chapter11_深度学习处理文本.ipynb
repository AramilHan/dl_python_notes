{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ecdb0d0-6fab-4176-a067-8083565198ed",
   "metadata": {},
   "source": [
    "# 本章摘要"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d3a383-2799-4530-bc77-60c300db0375",
   "metadata": {},
   "source": [
    "- 为机器学习应用预处理文本数据\n",
    "- 用于文本处理的词袋方法和序列模型方法\n",
    "- Transformer架构\n",
    "- 序列好序列学习"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166a35a4-64af-49a4-93fa-0908e0552c21",
   "metadata": {},
   "source": [
    "# 自然语言处理概述（NLP）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0969a897-d387-444b-b13e-899fe8b3fa15",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 准备文本数据"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ecc0a5-4450-4e8c-83f8-fa35e7e3a975",
   "metadata": {},
   "source": [
    "深度学习模型是可微函数，只能处理数值张量，不能将原始文本作为输入。**文本向量化**是指将文本转换为数值张量的过程。文本向量化有许多种形式，但都遵循相同的流程。<br>\n",
    "**文本向量化流程** <br>\n",
    "![从原始文本到向量](images/从原始文本到向量.png \"从原始文本到向量\") <p>\n",
    "1. 将文本标准化，比如：转换为小写字母或删除标点符号；\n",
    "2. 将文本拆分为单元[称为**词元**（token）]，比如字符、单词或词组。这一步叫作**词元化**；\n",
    "3. 将每个词元转换为一个数值向量。这通常需要对数据中的所有词元**建立索引**。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85fc3d69-5563-4126-8404-ab66ec6bde65",
   "metadata": {},
   "source": [
    "## 文本标准化"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220b0dbd-ad16-499a-93ec-b2cc5da5a678",
   "metadata": {},
   "source": [
    "文本标准化是一种简单的特征工程，旨在消除不希望模型处理的那些编码差异。<p>\n",
    "- 最简单也是最广泛使用的一种标准化方法是：将所有字母转换为小写并删除标点符号。\n",
    "- 更高级的标准化方法，但在机器学习中很少使用，**词干提取**。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2baee1-680f-4303-8bd5-3637390a1ac0",
   "metadata": {},
   "source": [
    "## 文本拆分（词元化）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c656527-9b67-45ce-a63f-2d42fdd7c98c",
   "metadata": {},
   "source": [
    "词元化有以下3种方法：\n",
    "- **单词级词元化**（word-level tokenization）：词元是以空格（或标点）分隔的子字符串。\n",
    "- **N元语法词元化**（N-gram tokenization）：词元是N个连续单词。\n",
    "- **字符级词元化**（character-level tokenization）：每个字符都是一个词元。<p>\n",
    "\n",
    "一般情况下，可以一直使用单词级词元化或N元语法词元化。有两种文本处理模型：\n",
    "- 一种是关注词序的模型，叫作**序列模型**（sequence model）；\n",
    "- 另一种将输入单词作为一个集合，不考虑其原始顺序，叫作**词袋模型**（bag-of-words model）。<p>\n",
    "\n",
    "如果要构建序列模型，则应使用单词级词元化；如果要构建词袋模型，则应使用N元语法词元化。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6af825-02c7-45f0-b235-49b0c8bf51a3",
   "metadata": {},
   "source": [
    "### 理解N元语法和词袋"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9aab5df-99b8-43f0-9834-f510225b3c22",
   "metadata": {},
   "source": [
    "对于句子“The cat sat on the mat”（猫坐在垫子上）。<br>\n",
    "分解为二元语法的集合 <br>\n",
    "```\n",
    "{\"the\", \"the cat\", \"cat\", \"cat sat\", \"sat\", \"sat on\", \"on\", \"on the\", \"the mat\", \"mat\"}\n",
    "```\n",
    "<p>\n",
    "\n",
    "分解为三元语法的集合 <br>\n",
    "```\n",
    "{\"the\", \"the cat\", \"cat\", \"cat sat\", \"the cat sat\", \"sat\", \"sat on\", \"on\", \"cat sat on\", \"on the\", \"sat on the\", \"the mat\", \"mat\", \"on the mat\"}\n",
    "```\n",
    "<p>\n",
    "\n",
    "这样的集合分别叫作**二元语法袋**（bag-of-2-grams）和**三元语法袋**（bag-of-3-grams）。**袋**是指，处理的是词元组成的集合，而不是列表或序列，也就是说，词元没有特定的顺序。这种词元化方法叫作**词袋**（bag-of-words）或**N元语法袋**（bag-of-N-grams）。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f802aa-9d71-48a6-a0b4-9546483cfb4d",
   "metadata": {},
   "source": [
    "## 建立词表索引"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e0ae67-87ed-4f09-9bb4-e8b347953aef",
   "metadata": {},
   "source": [
    "将每个词元编码为数值表示。需要建立训练数据中所有单词（“词表”）的索引，并为词表中的每个单词分配唯一整数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbdd4b0c-cbf2-46e9-9de5-42cb7f7d242d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = {}\n",
    "for text in dataset:\n",
    "    text = standardize(text)\n",
    "    tokens = tokenize(text)\n",
    "    for token in tokens:\n",
    "        if token not in vocabulary:\n",
    "            vocabulary[token] = len(vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b8fd97-5ca9-4e2e-b256-6e71110afa37",
   "metadata": {},
   "source": [
    "然后，将这个整数转换为神经网络能够处理的向量编码，比如：One-hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b6ef88eb-aa26-42c0-a09d-8918df44f684",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode_token(token):\n",
    "    vector = np.zeros((len(vocabulary),))\n",
    "    token_index = vocabulary[token]\n",
    "    vector[token_index] = 1\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a369c0d-8bb0-463d-9263-4465b322c465",
   "metadata": {},
   "source": [
    "## 使用TextVectorization层"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21948a7c-2009-494b-b3f4-536fe857a78e",
   "metadata": {},
   "source": [
    "### 原始准备文本数据流程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6a463ac4-831f-4d9a-ab9a-b92c8d165d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "class Vectorizer:\n",
    "    # 文本标准化\n",
    "    def standardize(self, text):\n",
    "        text = text.lower()\n",
    "        return \"\".join(char for char in text if char not in string.punctuation)\n",
    "\n",
    "    # 文本词元化\n",
    "    def tokenize(self, text):\n",
    "        text = self.standardize(text)\n",
    "        return text.split()\n",
    "\n",
    "    # 建立词表索引\n",
    "    def make_vocabulary(self, dataset):\n",
    "        self.vocabulary = {\"\": 0, \"[UNK]\": 1}\n",
    "        for text in dataset:\n",
    "            text = self.standardize(text)\n",
    "            tokens = self.tokenize(text)\n",
    "            for token in tokens:\n",
    "                if token not in self.vocabulary:\n",
    "                    self.vocabulary[token] = len(self.vocabulary)\n",
    "        self.inverse_vocabulary = dict((v, k) for k, v in self.vocabulary.items())\n",
    "\n",
    "    def encode(self, text):\n",
    "        text = self.standardize(text)\n",
    "        tokens = self.tokenize(text)\n",
    "        return [self.vocabulary.get(token, 1) for token in tokens]\n",
    "\n",
    "    def decode(self, int_sequence):\n",
    "        return \" \".join(self.inverse_vocabulary.get(i, \"[UNK]\") for i in int_sequence)\n",
    "\n",
    "\n",
    "vectorizer = Vectorizer()\n",
    "dataset = [\n",
    "    \"I write, erase, rewrite\",\n",
    "    \"Erase again, and then\",\n",
    "    \"A poppy blooms.\",\n",
    "]\n",
    "vectorizer.make_vocabulary(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "023b6b6d-1400-4947-b4f4-5359b63915b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 3, 5, 7, 1, 5, 6]\n",
      "i write rewrite and [UNK] rewrite again\n"
     ]
    }
   ],
   "source": [
    "test_sentence = \"I write, rewrite, and still rewrite again\"\n",
    "encode_sentence = vectorizer.encode(test_sentence)\n",
    "print(encode_sentence)\n",
    "decode_sentence = vectorizer.decode(encode_sentence)\n",
    "print(decode_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eddfb65-830d-4835-9129-43c5c63fcf7c",
   "metadata": {},
   "source": [
    "### 使用TextVectorization层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a72cf08b-58e5-4102-b0bc-19f29762004e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-10 11:10:41.476372: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M1 Pro\n",
      "2024-10-10 11:10:41.476421: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 16.00 GB\n",
      "2024-10-10 11:10:41.476439: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 5.33 GB\n",
      "2024-10-10 11:10:41.476490: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-10-10 11:10:41.476513: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import TextVectorization\n",
    "text_vectorization = TextVectorization(\n",
    "    output_mode=\"int\",  # 设置该层的返回值是编码为整数索引的单词序列\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8443a29-a2d2-4413-b30d-ae4f2cd57d5b",
   "metadata": {},
   "source": [
    "#### 自定义TextVectorization层标准化和词元化方法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f8bf55-bcb8-4333-9120-921549e31f3d",
   "metadata": {},
   "source": [
    "TextVectorization层默认的文本标准化方法是“转换为小写字母并删除标点符号”，词元化方法是“利用空格进行拆分”。也可以提供自定义函数来进行标准化和词元化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2dfd0b03-bd13-4cac-8460-6a52d3ed4e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import tensorflow as tf\n",
    "\n",
    "def custom_standardization_fn(string_tensor):\n",
    "    lowercase_string = tf.strings.lower(string_tensor)  # 将字符串转换为小写字母\n",
    "    return tf.strings.regex_replace(  # 将标点符号替换为空字符串\n",
    "        lowercase_string, f\"[{re.escape(string.punctuation)}]\", \"\")\n",
    "\n",
    "def custom_split_fn(string_tensor):\n",
    "    return tf.strings.split(string_tensor)  # 利用空格对字符串进行拆分\n",
    "\n",
    "text_vectorization = TextVectorization(\n",
    "    output_mode=\"int\",\n",
    "    standardize=custom_standardization_fn,\n",
    "    split=custom_split_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53cfabdf-687d-4147-9d00-78dfdfd306bd",
   "metadata": {},
   "source": [
    "#### 词表建立索引"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69c75bf-9611-48c9-9beb-7e5796a430a0",
   "metadata": {},
   "source": [
    "利用TextVectorization层对文本语料库的词表建立索引，需要调用`adapt()`方法，其参数是一个可以生成字符串的Dataset对象或一个由Python字符串组成的列表。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d3663999-6161-43af-9320-66890fe7a0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [\n",
    "    \"I write, erase, rewrite\",\n",
    "    \"Erase again, and then\",\n",
    "    \"A poppy blooms.\",\n",
    "]\n",
    "text_vectorization.adapt(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7360b6b3-a6d0-46ee-a1cf-8980168d1e68",
   "metadata": {},
   "source": [
    "##### 显示词表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d5dbfadc-1fb3-46b7-96a9-f61aaf085656",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '[UNK]',\n",
       " 'erase',\n",
       " 'write',\n",
       " 'then',\n",
       " 'rewrite',\n",
       " 'poppy',\n",
       " 'i',\n",
       " 'blooms',\n",
       " 'and',\n",
       " 'again',\n",
       " 'a']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_vectorization.get_vocabulary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c9ce08-e272-4892-855b-1788c212ee85",
   "metadata": {},
   "source": [
    "#### 对例句进行编码，然后解码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6215c4b3-1338-42a3-8bc2-973125899b5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([ 7  3  5  9  1  5 10], shape=(7,), dtype=int64)\n",
      "i write rewrite and [UNK] rewrite again\n"
     ]
    }
   ],
   "source": [
    "vocabulary = text_vectorization.get_vocabulary()\n",
    "test_sentence = \"I write, rewrite, and still rewrite again\"\n",
    "encoded_sentence = text_vectorization(test_sentence)\n",
    "print(encoded_sentence)\n",
    "inverse_vocab = dict(enumerate(vocabulary))\n",
    "decoded_sentence = \" \".join(inverse_vocab[int(i)] for i in encoded_sentence)\n",
    "print(decoded_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac18b614-9567-427f-a9e7-95afdba523c9",
   "metadata": {},
   "source": [
    "#### TextVectorization层在模型构建中的使用"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73cb6ae6-1d0a-463b-81b1-9809a0e597da",
   "metadata": {},
   "source": [
    "TextVectorization层有两种用法：\n",
    "- 将其放在`tf.data`管道中\n",
    "- 将其作为模型的一部分"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b12925-599f-47a5-9569-79fa19700ec1",
   "metadata": {},
   "source": [
    "##### 放在tf.data管道"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d4fb8b-f81e-4f97-8208-d8a466f365d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "int_sequence_dataset = string_dataset.map(\n",
    "    text_vectorization,\n",
    "    num_parallel_calls=4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d2ca77-f2dc-4b20-b9ea-f08fc901198a",
   "metadata": {},
   "source": [
    "##### 作为模型的一部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034fffe1-f58f-4410-a51e-fd2a45a8895b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_input = keras.Input(shape=(), dtype=\"string\")  # 创建输入的符号张量，数据类型为字符串\n",
    "vectorized_text = text_vectorization(text_input)  # 对输入应用文本向量化层\n",
    "embedded_input = keras.layers.Embedding(...)(vectorized_text)\n",
    "output = ...\n",
    "model = keras.Model(text_input, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff186c9-b03c-4bdb-991b-c7c8e47403c6",
   "metadata": {},
   "source": [
    "##### 两种方法的区别"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f06cd79-86e7-478c-b196-8e4470d14d3a",
   "metadata": {},
   "source": [
    "如果向量化是模型的一部分，那么它将与模型的其他部分同步进行。这意味着在每个训练步骤中，模型的其余部分（在GPU）必须等待TextVectorization层（在CPU）的输出准备好，才能开始工作。与此相对，如果将该层放在tf.data管道中，则可以在CPU上对数据进行异步预处理：模型在GPU上对一批向量化数据进行处理时，CPU可以对下一批原始字符串进行向量化。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b976f4ea-8029-42ae-b7ef-a395938f7eaf",
   "metadata": {},
   "source": [
    "# 表示单词组的两种方法：集合和序列"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e497be-741a-4e7a-999a-fc0b21f3a05c",
   "metadata": {},
   "source": [
    "机器学习模型如何表示**单个单词**：它是分类特征（来自预定义集合的值）。它应该被编码为特征空间中的维度，或者类别向量。**如何对单词组成句子的方式进行编码？** <p>\n",
    "\n",
    "如何表示词序是一个关键问题。<br>\n",
    "- 最简单的做法是舍弃顺序，将文本看作一组无序的单词，这就是**词袋模型**(bag-of-words model)。<br>\n",
    "- 也可以严格按照单词出现顺序进行处理，一次处理一个，就像处理时间序列的时间步一样，也就是利用**RNN模型**。<br>\n",
    "- 也可以采用混合方法：Transformer架构在技术上是不考虑顺序的，但它将单词位置信息注入数据表示中，从而能够同时查看一个句子的不同部分。<br>\n",
    "\n",
    "RNN和Transformer被称为**序列模型**。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7878e9bc-0be4-40cc-bda4-26e1253e657a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 准备IMDB影评数据"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f911f01-3ab3-43b2-b841-ce3937123fda",
   "metadata": {},
   "source": [
    "### 构建验证集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f3f5a246-a1ea-4c13-a956-f0aaeb3fd596",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将20%的训练文本文件放入一个新目录\n",
    "import os, pathlib,shutil, random\n",
    "\n",
    "base_dir = pathlib.Path(\"aclImdb/\")\n",
    "val_dir = base_dir / \"val\"\n",
    "train_dir = base_dir / \"train\"\n",
    "for category in (\"neg\", \"pos\"):\n",
    "    os.makedirs(val_dir / category)\n",
    "    files = os.listdir(train_dir / category)\n",
    "    random.Random(1337).shuffle(files)\n",
    "    num_val_samples = int(0.2 * len(files))\n",
    "    val_files = files[-num_val_samples:]\n",
    "    for fname in val_files:\n",
    "        shutil.move(train_dir / category / fname, val_dir/ category / fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c9d2a2-0f29-47d4-9c48-5ea8ee86cd3d",
   "metadata": {},
   "source": [
    "### 创建批量Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "8874ca6e-f49f-4c4a-b843-54c2ed5132b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20000 files belonging to 2 classes.\n",
      "Found 5000 files belonging to 2 classes.\n",
      "Found 25000 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "batch_size = 32\n",
    "train_ds = keras.utils.text_dataset_from_directory(\n",
    "    \"aclImdb/train\", batch_size=batch_size)\n",
    "val_ds = keras.utils.text_dataset_from_directory(\n",
    "    \"aclImdb/val/\", batch_size=batch_size)\n",
    "test_ds = keras.utils.text_dataset_from_directory(\n",
    "    \"aclImdb/test/\", batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "9322b9ef-0e33-4fc8-805c-0f78813f3721",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs shape: (32,)\n",
      "inputs dtype: <dtype: 'string'>\n",
      "targets shape: (32,)\n",
      "targets dtype: <dtype: 'int32'>\n",
      "inputs[0]: tf.Tensor(b'Yeah. Pretty sure I saw this movie years ago when it was about the Supremes.<br /><br />Another recycled storyline glitzed up Hollywood-style, borrowing scripts from better making-it-in-the-music-industry films.<br /><br />Nothing original here.<br /><br />More make-up, glammier costumes and choreography = more money for the questionably \"talented\" Beyonce draw.<br /><br />If you like the throwback style, you should appreciate actual groups who struggled (without having digitized voices and a Hollywood empire).<br /><br />Beyonce\\'s involvement makes this hypocritical garbage.', shape=(), dtype=string)\n",
      "targets[0]: tf.Tensor(0, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for inputs, targets in train_ds:\n",
    "    print(\"inputs shape:\", inputs.shape)\n",
    "    print(\"inputs dtype:\", inputs.dtype)\n",
    "    print(\"targets shape:\", targets.shape)\n",
    "    print(\"targets dtype:\", targets.dtype)\n",
    "    print(\"inputs[0]:\", inputs[0])\n",
    "    print(\"targets[0]:\", targets[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8cde76-ce1f-4c19-89a7-d8b9fafb6cb8",
   "metadata": {},
   "source": [
    "## 将单词作为集合处理：词袋方法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c8b410-f2c5-4289-b06e-dc99c6218bda",
   "metadata": {},
   "source": [
    "要对一段文本进行编码，使其可以被机器学习模型所处理，最简单的方法是**舍弃顺序**，将文本看作一组（一袋）词元。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782661a8-2135-42b5-9643-022ec6d3686f",
   "metadata": {},
   "source": [
    "### 单个单词（一元语法）的二进制编码"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375606d3-39b2-4995-a731-620c6e63c6fa",
   "metadata": {},
   "source": [
    "如果使用单个单词的词袋，那么“the cat sat on the mat”这个句子就会变成`{\"cat\", \"mat\", \"on\", \"sat\", \"the\"}`。 <p>\n",
    "\n",
    "这种编码方式的主要优点是，可以将整个文本表示为单一向量，其中每个元素表示某个单词是否存在。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defec129-fcf8-46c7-9038-328317b178de",
   "metadata": {},
   "source": [
    "#### 用TextVectorization层预处理数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "24914629-45be-4194-83ff-183bbcee6b25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-12 15:41:42.475325: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "text_vectorization = TextVectorization(\n",
    "    max_tokens=20000,\n",
    "    output_mode=\"multi_hot\"\n",
    ")\n",
    "text_only_train_ds = train_ds.map(lambda x, y: x)  # 准备一个数据集，只包含原始文本输入\n",
    "text_vectorization.adapt(text_only_train_ds)  # 利用adapt()方法对数据集词表建立索引\n",
    "binary_1gram_train_ds = train_ds.map(  # 分别对训练、验证和测试数据集进行处理\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4\n",
    ")\n",
    "binary_1gram_val_ds = val_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4\n",
    ")\n",
    "binary_1gram_test_df = test_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffef0811-9ac7-493a-ae88-4bb512ff4255",
   "metadata": {},
   "source": [
    "#### 查看一元语法二进制数据集的输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "7e55a69c-8a8b-47d8-a49a-44def7c9b18c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs shape: (32, 20000)\n",
      "inputs dtype: <dtype: 'int64'>\n",
      "targets shape: (32,)\n",
      "targets dtype: <dtype: 'int32'>\n",
      "inputs[0]: tf.Tensor([1 1 1 ... 0 0 0], shape=(20000,), dtype=int64)\n",
      "targets[0]: tf.Tensor(1, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for inputs, targets in binary_1gram_train_ds:\n",
    "    print(\"inputs shape:\", inputs.shape)\n",
    "    print(\"inputs dtype:\", inputs.dtype)\n",
    "    print(\"targets shape:\", targets.shape)\n",
    "    print(\"targets dtype:\", targets.dtype)\n",
    "    print(\"inputs[0]:\", inputs[0])\n",
    "    print(\"targets[0]:\", targets[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a29259e-8aea-469e-a8b5-9995d7198d40",
   "metadata": {},
   "source": [
    "#### 模型构建函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "c9edc3db-70a0-4e95-92b4-0230724d133c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "def get_model(max_tokens=20000, hidden_dim=16):\n",
    "    inputs = keras.Input(shape=(max_tokens,))\n",
    "    x = layers.Dense(hidden_dim, activation=\"relu\")(inputs)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    model.compile(optimizer=\"rmsprop\",\n",
    "                  loss=\"binary_crossentropy\",\n",
    "                  metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47198499-a5fe-4901-894c-f57cbeeee482",
   "metadata": {},
   "source": [
    "#### 对一元语法二进制进行训练和测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "e1196903-e282-4865-9396-f9a6c2d2d756",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20000</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │       <span style=\"color: #00af00; text-decoration-color: #00af00\">320,016</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_1 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20000\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │       \u001b[38;5;34m320,016\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m17\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">320,033</span> (1.22 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m320,033\u001b[0m (1.22 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">320,033</span> (1.22 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m320,033\u001b[0m (1.22 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-12 15:52:45.493519: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m204s\u001b[0m 322ms/step - accuracy: 0.7601 - loss: 0.5093 - val_accuracy: 0.8916 - val_loss: 0.2819\n",
      "Epoch 2/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - accuracy: 0.8857 - loss: 0.2982 - val_accuracy: 0.9022 - val_loss: 0.2646\n",
      "Epoch 3/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - accuracy: 0.9058 - loss: 0.2545 - val_accuracy: 0.9002 - val_loss: 0.2767\n",
      "Epoch 4/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - accuracy: 0.9178 - loss: 0.2274 - val_accuracy: 0.8952 - val_loss: 0.2853\n",
      "Epoch 5/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - accuracy: 0.9256 - loss: 0.2151 - val_accuracy: 0.8978 - val_loss: 0.3027\n",
      "Epoch 6/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - accuracy: 0.9251 - loss: 0.2159 - val_accuracy: 0.8962 - val_loss: 0.3147\n",
      "Epoch 7/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - accuracy: 0.9306 - loss: 0.2042 - val_accuracy: 0.8940 - val_loss: 0.3178\n",
      "Epoch 8/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - accuracy: 0.9315 - loss: 0.2070 - val_accuracy: 0.8912 - val_loss: 0.3307\n",
      "Epoch 9/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - accuracy: 0.9350 - loss: 0.2019 - val_accuracy: 0.8928 - val_loss: 0.3359\n",
      "Epoch 10/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - accuracy: 0.9408 - loss: 0.1917 - val_accuracy: 0.8940 - val_loss: 0.3474\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m189s\u001b[0m 241ms/step - accuracy: 0.8871 - loss: 0.2918\n",
      "Test acc: 0.888\n"
     ]
    }
   ],
   "source": [
    "model = get_model()\n",
    "model.summary()\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\"binary_1gram.keras\", save_best_only=True)\n",
    "]\n",
    "model.fit(binary_1gram_train_ds.cache(),\n",
    "          validation_data=binary_1gram_val_ds.cache(),\n",
    "          epochs=10,\n",
    "          callbacks=callbacks)\n",
    "model = keras.models.load_model(\"binary_1gram.keras\")\n",
    "print(f\"Test acc: {model.evaluate(binary_1gram_test_df)[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73197dc4-6dd4-4266-8ca6-d64c877d3efe",
   "metadata": {},
   "source": [
    "### 二元语法的二进制编码"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e4409d-f199-4876-89da-1a4599ca5461",
   "metadata": {},
   "source": [
    "#### 设置TextVectorization层返回二元语法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "8274d485-bcab-4e28-9bae-5360a383f719",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorization = TextVectorization(\n",
    "    ngrams=2,\n",
    "    max_tokens=20000,\n",
    "    output_mode=\"multi_hot\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38fe8f4-0505-4cf0-975c-91795f36ed87",
   "metadata": {},
   "source": [
    "#### 对二元语法二进制模型进行训练和测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "4f1d0b67-5960-4837-bfc6-52a698d96ebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-12 17:12:55.711989: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20000</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │       <span style=\"color: #00af00; text-decoration-color: #00af00\">320,016</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_2 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20000\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │       \u001b[38;5;34m320,016\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m17\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">320,033</span> (1.22 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m320,033\u001b[0m (1.22 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">320,033</span> (1.22 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m320,033\u001b[0m (1.22 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m616s\u001b[0m 980ms/step - accuracy: 0.7902 - loss: 0.4621 - val_accuracy: 0.9060 - val_loss: 0.2496\n",
      "Epoch 2/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m580s\u001b[0m 924ms/step - accuracy: 0.9130 - loss: 0.2500 - val_accuracy: 0.9062 - val_loss: 0.2511\n",
      "Epoch 3/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m558s\u001b[0m 890ms/step - accuracy: 0.9311 - loss: 0.2001 - val_accuracy: 0.9042 - val_loss: 0.2567\n",
      "Epoch 4/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m544s\u001b[0m 867ms/step - accuracy: 0.9428 - loss: 0.1806 - val_accuracy: 0.9010 - val_loss: 0.2823\n",
      "Epoch 5/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m538s\u001b[0m 857ms/step - accuracy: 0.9488 - loss: 0.1689 - val_accuracy: 0.9002 - val_loss: 0.2914\n",
      "Epoch 6/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m523s\u001b[0m 834ms/step - accuracy: 0.9511 - loss: 0.1680 - val_accuracy: 0.8952 - val_loss: 0.3136\n",
      "Epoch 7/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m540s\u001b[0m 862ms/step - accuracy: 0.9550 - loss: 0.1541 - val_accuracy: 0.8952 - val_loss: 0.3251\n",
      "Epoch 8/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m537s\u001b[0m 856ms/step - accuracy: 0.9586 - loss: 0.1670 - val_accuracy: 0.8938 - val_loss: 0.3311\n",
      "Epoch 9/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m528s\u001b[0m 841ms/step - accuracy: 0.9588 - loss: 0.1393 - val_accuracy: 0.8964 - val_loss: 0.3478\n",
      "Epoch 10/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m528s\u001b[0m 843ms/step - accuracy: 0.9597 - loss: 0.1459 - val_accuracy: 0.8956 - val_loss: 0.3413\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m519s\u001b[0m 659ms/step - accuracy: 0.8968 - loss: 0.2657\n",
      "Test acc: 0.897\n"
     ]
    }
   ],
   "source": [
    "text_vectorization.adapt(text_only_train_ds)\n",
    "binary_2gram_train_ds = train_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4\n",
    ")\n",
    "binary_2gram_val_ds = val_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4\n",
    ")\n",
    "binary_2gram_test_ds = test_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4\n",
    ")\n",
    "model = get_model()\n",
    "model.summary()\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\"binary_2gram.keras\", save_best_only=True)\n",
    "]\n",
    "model.fit(\n",
    "    binary_2gram_train_ds,\n",
    "    validation_data=binary_2gram_val_ds,\n",
    "    epochs=10,\n",
    "    callbacks=callbacks\n",
    ")\n",
    "model = keras.models.load_model(\"binary_2gram.keras\")\n",
    "print(f\"Test acc: {model.evaluate(binary_2gram_test_ds)[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278f8f83-d1a1-4012-ad59-f6d9259a6d45",
   "metadata": {},
   "source": [
    "### 二元语法的TF-IDF编码"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3911283b-399f-42fe-ab8f-2adda89cf0cd",
   "metadata": {},
   "source": [
    "为二元语法添加更多的信息，方法就是计算每个单词或者每个N元语法的出现次数。 <br>\n",
    "```\n",
    "{\"the\": 2, \"the cat\": 1, \"cat\": 1, \"cat sat\": 1, \"sat\": 1,\n",
    "\"sat on\": 1, \"on\": 1, \"on the\": 1, \"the mat\": 1, \"mat\": 1}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb43f7a3-6bcd-4c15-9572-8dcf37283b38",
   "metadata": {},
   "source": [
    "#### 设置TextVectorization层返回词元出现次数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "0240d58f-eba8-452c-a637-4f60f2703191",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorization = TextVectorization(\n",
    "    ngrams=2,\n",
    "    max_tokens=20000,\n",
    "    output_mode=\"count\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273cfcce-f95c-4c4e-82d3-8065830c6bba",
   "metadata": {},
   "source": [
    "#### 设置TextVectorization层返回TF-IDF加权输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "c5b3b952-c741-40a0-a88f-a2cf09fb9ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorization = TextVectorization(\n",
    "    ngrams=2,\n",
    "    max_tokens=20000,\n",
    "    output_mode=\"tf_idf\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc94e5c9-3a56-4c19-bc29-194df5ea160a",
   "metadata": {},
   "source": [
    "#### 理解TF-IDF规范化"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f8bbbb-466d-4e89-87b6-7833666ce404",
   "metadata": {},
   "source": [
    "某个词在一个文档中出现的次数越多，它对理解文档的内容就越重要。同时，某个词在数据集所有文档中的出现频次也很重要：如果一个词几乎出现在每个文档中(比如“the”或“a”)，那么这个词就不是特别有信息量，而仅在一小部分文本中出现的词(比如“Herzog”)则是非常独特的，因此也非常重要。TF-IDF指标融合了这两种思想。它将某个词的“词频”除以“文档频次”，前者是该词在当前文档中的出现次数，后者是该词在整个数据集中的出现频次。TF-IDF计算方法如下。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "8cde78b0-9871-4dbe-bbc4-fb63f27f3e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf(term, document, dataset):\n",
    "    term_freq = document.count(term)\n",
    "    doc_freq = math.log(sum(doc.count(term) for doc in dataset) + 1)\n",
    "    return term_freq / doc_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "afbd956c-c8b1-4e10-87a5-1a79add71827",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_only_train_ds = text_only_train_ds.map(lambda x: x[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2e552a-0b5b-498f-8b6c-ec32aa86f0ed",
   "metadata": {},
   "source": [
    "#### 对TF-IDF二元语法模型进行训练和测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17db5c76-79ff-475c-8bef-7b40c99bdbaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-15 15:15:19.166764: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_3\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_3\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20000</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │       <span style=\"color: #00af00; text-decoration-color: #00af00\">320,016</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_3 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20000\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │       \u001b[38;5;34m320,016\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m17\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">320,033</span> (1.22 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m320,033\u001b[0m (1.22 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">320,033</span> (1.22 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m320,033\u001b[0m (1.22 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m365/625\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m3:43\u001b[0m 860ms/step - accuracy: 0.6545 - loss: 0.9173"
     ]
    }
   ],
   "source": [
    "text_vectorization.adapt(text_only_train_ds)\n",
    "tfidf_2gram_train_ds = train_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4\n",
    ")\n",
    "tfidf_2gram_val_ds = val_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4\n",
    ")\n",
    "tfidf_2gram_test_ds = test_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4\n",
    ")\n",
    "\n",
    "model = get_model()\n",
    "model.summary()\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\"tfidf_2gram.keras\", save_best_only=True)\n",
    "]\n",
    "model.fit(\n",
    "    tfidf_2gram_train_ds.cache(),\n",
    "    validation_data=tfidf_2gram_val_ds.cache(),\n",
    "    epochs=10,\n",
    "    callbacks=callbacks\n",
    ")\n",
    "model = keras.models.load_model(\"tfidf_2gram.keras\")\n",
    "print(f\"Test acc: {model.evaluate(tfidf_2gram_test_ds)[1]:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python(hjw)",
   "language": "python",
   "name": "hjw"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
